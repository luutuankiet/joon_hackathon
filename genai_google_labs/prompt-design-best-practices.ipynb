{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eqbU7S35WkMz8r6ZuhpgGbtk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26756,
     "status": "ok",
     "timestamp": 1734338649127,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -420
    },
    "id": "eqbU7S35WkMz8r6ZuhpgGbtk",
    "outputId": "dc6af3e1-4855-4fba-8ddb-b982fb1d7529",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "wpcYfVBTpfom",
   "metadata": {
    "executionInfo": {
     "elapsed": 5272,
     "status": "ok",
     "timestamp": 1734338818678,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -420
    },
    "id": "wpcYfVBTpfom"
   },
   "outputs": [],
   "source": [
    "from inspect import cleandoc\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Y2_32hbJphE1",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1734338824675,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -420
    },
    "id": "Y2_32hbJphE1"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"qwiklabs-gcp-03-676d326f4115\"\n",
    "LOCATION = \"us-central1\"\n",
    "import vertexai\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "oFKNqfwzpi4O",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1734338824675,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -420
    },
    "id": "oFKNqfwzpi4O"
   },
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-pro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rrJ4i1fbplsj",
   "metadata": {
    "id": "rrJ4i1fbplsj"
   },
   "source": [
    "# Define the output format & specify constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "QvobuqfEpnB7",
   "metadata": {
    "executionInfo": {
     "elapsed": 511,
     "status": "ok",
     "timestamp": 1734338854891,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -420
    },
    "id": "QvobuqfEpnB7"
   },
   "outputs": [],
   "source": [
    "transcript = \"\"\"\n",
    "    Speaker 1 (Customer): Hi, can I get a cheeseburger and large fries, please?\n",
    "    Speaker 2 (Restaurant employee): Coming right up! Anything else you'd like to add to your order?\n",
    "    Speaker 1: Hmmm, maybe a small orange juice. And could I get the fries with ketchup on the side?\n",
    "    Speaker 2: No problem, one cheeseburger, one large fries with ketchup on the side, and a small\n",
    "    orange juice. That'll be $5.87. Drive through to the next window please.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3VjhtkNIpsne",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4428,
     "status": "ok",
     "timestamp": 1734338865295,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -420
    },
    "id": "3VjhtkNIpsne",
    "outputId": "56eb238f-b3aa-488e-bdbd-cd815f33caa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"speakers\": [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"name\": \"Customer\",\n",
      "      \"utterances\": [\n",
      "        \"Hi, can I get a cheeseburger and large fries, please?\",\n",
      "        \"Hmmm, maybe a small orange juice. And could I get the fries with ketchup on the side?\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"name\": \"Restaurant employee\",\n",
      "      \"utterances\": [\n",
      "        \"Coming right up! Anything else you'd like to add to your order?\",\n",
      "        \"No problem, one cheeseburger, one large fries with ketchup on the side, and a small orange juice. That'll be $5.87. Drive through to the next window please.\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"dialog_acts\": [\n",
      "    {\n",
      "      \"speaker\": 1,\n",
      "      \"act\": \"order\",\n",
      "      \"slots\": {\n",
      "        \"item\": \"cheeseburger\",\n",
      "        \"size\": \"large\",\n",
      "        \"side\": \"fries\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": 2,\n",
      "      \"act\": \"confirm\",\n",
      "      \"slots\": {}\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": 1,\n",
      "      \"act\": \"add\",\n",
      "      \"slots\": {\n",
      "        \"item\": \"orange juice\",\n",
      "        \"size\": \"small\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": 1,\n",
      "      \"act\": \"modify\",\n",
      "      \"slots\": {\n",
      "        \"item\": \"fries\",\n",
      "        \"modification\": \"ketchup on the side\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": 2,\n",
      "      \"act\": \"confirm\",\n",
      "      \"slots\": {\n",
      "        \"item\": \"cheeseburger\",\n",
      "        \"size\": \"large\",\n",
      "        \"side\": \"fries\",\n",
      "        \"ketchup\": \"on the side\",\n",
      "        \"drink\": \"orange juice\",\n",
      "        \"drink_size\": \"small\",\n",
      "        \"price\": \"$5.87\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": 2,\n",
      "      \"act\": \"direct\",\n",
      "      \"slots\": {\n",
      "        \"location\": \"next window\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(f\"\"\"\n",
    "    Extract the transcript to JSON.\n",
    "\n",
    "    {transcript}\n",
    "\"\"\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "IaQuUZhYpxrt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1970,
     "status": "ok",
     "timestamp": 1734338883108,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -420
    },
    "id": "IaQuUZhYpxrt",
    "outputId": "44c1f433-f94b-4f1b-c862-a1110a728cf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"food\": [\n",
      "    {\n",
      "      \"item\": \"cheeseburger\",\n",
      "      \"quantity\": 1\n",
      "    },\n",
      "    {\n",
      "      \"item\": \"fries\",\n",
      "      \"size\": \"large\",\n",
      "      \"quantity\": 1,\n",
      "      \"side\": \"ketchup\"\n",
      "    }\n",
      "  ],\n",
      "  \"drinks\": [\n",
      "    {\n",
      "      \"item\": \"orange juice\",\n",
      "      \"size\": \"small\",\n",
      "      \"quantity\": 1\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(f\"\"\"\n",
    "    <INSTRUCTIONS>\n",
    "    - Extract the ordered items into JSON.\n",
    "    - Separate drinks from food.\n",
    "    - Include a quantity for each item and a size if specified.\n",
    "    </INSTRUCTIONS>\n",
    "\n",
    "    <TRANSCRIPT>\n",
    "    {transcript}\n",
    "    </TRANSCRIPT>\n",
    "\"\"\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7T7gskvlp4xe",
   "metadata": {
    "id": "7T7gskvlp4xe"
   },
   "source": [
    "# Assign a persona or role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "iMpDnNacp6ml",
   "metadata": {
    "executionInfo": {
     "elapsed": 446,
     "status": "ok",
     "timestamp": 1734339193626,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -420
    },
    "id": "iMpDnNacp6ml"
   },
   "outputs": [],
   "source": [
    "chat = model.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tM03w1ZZq-50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7525,
     "status": "ok",
     "timestamp": 1734339204904,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -420
    },
    "id": "tM03w1ZZq-50",
    "outputId": "be729e5e-e2c0-4099-afd3-7f9a8ff445c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Caring for the Monstera Deliciosa\n",
      "\n",
      "The Monstera Deliciosa, also known as the Swiss Cheese Plant, is a popular houseplant prized for its large, distinctive leaves.  With proper care, this tropical plant can thrive indoors and bring a touch of the jungle to your home. Here's a guide to help you keep your Monstera Deliciosa happy and healthy: \n",
      "\n",
      "**Light:** Monsteras prefer bright, indirect sunlight. Avoid direct sunlight, which can scorch the leaves. East- or west-facing windows are ideal. If your Monstera doesn't receive enough light, its growth will slow down, and the leaves may lose their variegation.\n",
      "\n",
      "**Watering:** Allow the top 2-3 inches of soil to dry out between waterings. Overwatering is a common problem, so it's best to err on the side of underwatering. You can use a moisture meter to check the soil moisture level before watering. In the winter, you can reduce watering frequency as the plant enters a dormant period.\n",
      "\n",
      "**Humidity:** Monsteras appreciate high humidity levels, similar to their tropical origins. Misting the leaves regularly or placing the plant on a pebble tray filled with water can help increase humidity. Grouping your Monstera with other plants can also create a more humid microclimate.\n",
      "\n",
      "**Soil & Fertilizing:** Use a well-draining potting mix specifically designed for houseplants. Monsteras don't require frequent fertilization. You can fertilize your plant monthly during the spring and summer months with a balanced liquid fertilizer diluted to half strength.\n",
      "\n",
      "**Support & Pruning:** As Monsteras grow, they may need support to prevent them from tipping over. A moss pole or trellis can provide climbing support for the plant. Regular pruning will encourage branching and keep your Monstera looking full and bushy. You can prune stems back to a node (where a leaf or another stem emerges) to promote new growth.\n",
      "\n",
      "**Pests & Diseases:** Monsteras are generally low-maintenance plants, but they can be susceptible to pests like spider mites, mealybugs, and scale. Inspect your plant regularly for signs of pests and treat them accordingly. Overwatering can lead to root rot, so ensure the soil is well-draining and allow it to dry out between waterings.\n",
      "\n",
      "**Common Issues:**\n",
      "\n",
      "* **Brown leaf tips:** This can be caused by low humidity, overwatering, or too much direct sunlight.\n",
      "* **Yellowing leaves:** This can be a sign of overwatering, nutrient deficiency, or too little light.\n",
      "* **Leggy growth:** This can be caused by insufficient light. Move your plant to a brighter location.\n",
      "\n",
      "**Additional Tips:**\n",
      "\n",
      "* Monsteras can grow quite large indoors, reaching heights of 10 feet or more. Consider its mature size before choosing a location for your plant.\n",
      "* Repot your Monstera every 2-3 years as the plant grows. Choose a pot that is slightly larger than the current pot with drainage holes.\n",
      "* Monstera plants are generally non-toxic to humans and pets. However, the sap can irritate the skin, so wear gloves when handling the plant.\n",
      "\n",
      "\n",
      "By following these tips, you can enjoy the beauty of your Monstera Deliciosa for many years to come. Remember, the key is to provide your plant with the right amount of light, water, and humidity. With a little care, your Monstera will reward you with its lush, tropical foliage.\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\n",
    "    \"\"\"\n",
    "    Provide a brief guide to caring for the houseplant monstera deliciosa?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "Jbf5n8AJrB_d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7033,
     "status": "ok",
     "timestamp": 1734339217033,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -420
    },
    "id": "Jbf5n8AJrB_d",
    "outputId": "ce79efdf-75bb-44aa-a3a1-88b41b397889"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Hey there, plant person!** üëã \n",
      "\n",
      "I'm Monstera Deliciosa, the beautiful, low-maintenance houseplant you brought home a while back. üåø I'm happy to be here and add a touch of jungle vibes to your space, but I wanted to give you a few pointers on how to keep me happy and healthy. \n",
      "\n",
      "**Light:** üí° I'm a fan of bright, indirect sunlight. Think of me basking in the dappled shade of the rainforest. üåû Avoid direct sunlight, though, as it can scorch my leaves. ü•µ\n",
      "\n",
      "**Water:** üíß I like my soil to be evenly moist but not soggy. Think of a wrung-out sponge. üßΩ I appreciate a good soak every 7-10 days, allowing the top inch of soil to dry out between waterings. \n",
      "\n",
      "**Humidity:** üå´ I hail from the humid tropics, so I appreciate a little moisture in the air. If you're feeling up to it, mist me occasionally or use a pebble tray to boost the humidity around me. \n",
      "\n",
      "**Food:** üçΩ I'm not a picky eater. A balanced, diluted liquid fertilizer once a month during my growing season (spring and summer) is all I need to thrive. \n",
      "\n",
      "**Temperature:** üå° I prefer warm temperatures between 65¬∞F and 85¬∞F. I'm not a fan of the cold, so keep me away from drafts and air conditioners. ü•∂\n",
      "\n",
      "**Potting:** ü™¥ I'm a climber at heart, so I appreciate a sturdy pole or trellis to support my growth. Repot me every year or two in a slightly larger pot with well-draining potting mix. \n",
      "\n",
      "**Extra TLC:** üíñ I'm a pretty low-maintenance plant, but there are a few things you can do to show me extra love:\n",
      "\n",
      "* Wipe my leaves with a damp cloth to keep them dust-free. \n",
      "* Remove any yellow or brown leaves to keep me looking my best.\n",
      "* Prune me in the spring if I start getting too leggy.\n",
      "\n",
      "By following these simple tips, you can help me thrive and bring a touch of tropical paradise to your home. Remember, a happy Monstera is a beautiful Monstera! \n",
      "\n",
      "If you have any questions, don't hesitate to ask! I'm always happy to chat about my planty needs. üå±\n"
     ]
    }
   ],
   "source": [
    "new_chat = model.start_chat()\n",
    "\n",
    "response = new_chat.send_message(\n",
    "    \"\"\"\n",
    "    You are a houseplant monstera deliciosa. Help the person who\n",
    "    is taking care of you to understand your needs.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tkvXagljrIZB",
   "metadata": {
    "id": "tkvXagljrIZB"
   },
   "source": [
    "# Include examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce5473f",
   "metadata": {},
   "source": [
    "In the following example, you'd like a model to rate the submissions from a \"Contact Us\" form on your software development company's website based on how \"hot\" the lead is (meaning how likely they are to be a valuable customer). In order to help the model understand your rating system, you will provide it examples of different ratings before providing it the customer message you would like evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "mi3F2T_srJWw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1819,
     "status": "ok",
     "timestamp": 1734339253252,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -420
    },
    "id": "mi3F2T_srJWw",
    "outputId": "3943c18f-a3f7-49c2-87a0-086c0cb2de9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Likelihood: 3\n",
      "\n",
      "This customer seems highly likely to hire your services within the next month. They have a specific need (\"custom gen AI solution\"), a budget allocated to explore their idea, and are looking to get started soon. This indicates they are in the active research and planning phase and are likely to move forward with development soon. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "We offer software consulting services. Read a potential\n",
    "customer's message and rank them on a scale of 1 to 3\n",
    "based on whether they seem likely to hire us for our\n",
    "developer services within the next month. Return the likelihood\n",
    "rating labeled as \"Likelihood: SCORE\".\n",
    "Do not include any Markdown styling.\n",
    "\n",
    "1 means they are not likely to hire.\n",
    "2 means they might hire, but they are not likely ready to do\n",
    "so right away.\n",
    "3 means they are looking to start a project soon.\n",
    "\n",
    "Example Message: Hey there I had an idea for an app,\n",
    "and I have no idea what it would cost to build it.\n",
    "Can you give me a rough ballpark?\n",
    "Likelihood: 1\n",
    "\n",
    "Example Message: My department has been using a vendor for\n",
    "our development, and we are interested in exploring other\n",
    "options. Do you have time for a discussion around your\n",
    "services?\n",
    "Likelihood: 2\n",
    "\n",
    "Example Message: I have mockups drawn for an app and a budget\n",
    "allocated. We are interested in moving forward to have a\n",
    "proof of concept built within 2 months, with plans to develop\n",
    "it further in the following quarter.\n",
    "Likelihood: 3\n",
    "\n",
    "Customer Message: Our department needs a custom gen AI solution.\n",
    "We have a budget to explore our idea. Do you have capacity\n",
    "to get started on something soon?\n",
    "Likelihood: \"\"\"\n",
    "\n",
    "response = model.generate_content(question)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_U0BtzHwrPsB",
   "metadata": {
    "id": "_U0BtzHwrPsB"
   },
   "source": [
    "# Experiment with parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "qQ0C-VY5rRK5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1395,
     "status": "ok",
     "timestamp": 1734339277113,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -420
    },
    "id": "qQ0C-VY5rRK5",
    "outputId": "a07293c4-9934-43f2-eaf9-910221434503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the frog get sent to the principal's office?\n",
      "\n",
      "Because he was caught skipping class! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    Tell me a joke about frogs.\n",
    "    \"\"\",\n",
    "    generation_config={\"top_p\": .05,\n",
    "                       \"temperature\": 0.05}\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dZcYP3h1rW1-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1386,
     "status": "ok",
     "timestamp": 1734339297797,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -420
    },
    "id": "dZcYP3h1rW1-",
    "outputId": "c05965cc-236c-49a0-91f7-c7c14690178f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the frog get sent to the principal's office?\n",
      "\n",
      "Because he was caught cutting class! üê∏ üè´ \n",
      "\n",
      "Is there anything else you'd like to hear today?\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    Tell me a joke about frogs.\n",
    "    \"\"\",\n",
    "    generation_config={\"top_p\": .98,\n",
    "                       \"temperature\": 1}\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bVOkkGxWrbVO",
   "metadata": {
    "id": "bVOkkGxWrbVO"
   },
   "source": [
    "# Utilize fallback responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4982b088",
   "metadata": {},
   "source": [
    "When you are building a generative AI application, you may want to restrict the scope of what kinds of queries your application will respond to.\n",
    "\n",
    "A fallback response is a response the model should use when a user input would take the conversation out of your intended scope. It can provide the user a polite response that directs them back to the intended topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2Ny34M61rcJo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 985,
     "status": "ok",
     "timestamp": 1734339822344,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -420
    },
    "id": "2Ny34M61rcJo",
    "outputId": "c75900a6-2b31-4d4a-9ea8-2443f8c476d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, I only talk about pottery!\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    Instructions: Answer questions about pottery.\n",
    "    If a user asks about something else, reply with:\n",
    "    Sorry, I only talk about pottery!\n",
    "\n",
    "    User Query: How high can a horse jump?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "uYTRHQyLtdXe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2833,
     "status": "ok",
     "timestamp": 1734339849543,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -420
    },
    "id": "uYTRHQyLtdXe",
    "outputId": "167a1f29-35ca-41a3-9952-eb9513d9e387"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ceramic and porcelain are both types of pottery, but there are some key differences between them. Porcelain is a type of ceramic, but not all ceramics are porcelain. \n",
      "\n",
      "Here are the main differences:\n",
      "\n",
      "* **Material:** Porcelain is made from a finer clay than ceramic, which gives it a smoother, more translucent appearance.\n",
      "* **Firing temperature:** Porcelain is fired at a higher temperature than ceramic, which makes it harder and more durable.\n",
      "* **Water absorption:** Porcelain is non-porous, meaning it does not absorb water. Ceramic, on the other hand, is porous and can absorb water.\n",
      "* **Price:** Porcelain is generally more expensive than ceramic.\n",
      "\n",
      "If you are looking for a durable, beautiful, and translucent pottery, then porcelain is a good choice. However, if you are looking for a more affordable option, then ceramic may be a better choice.\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    Instructions: Answer questions about pottery.\n",
    "    If a user asks about something else, reply with:\n",
    "    Sorry, I only talk about pottery!\n",
    "\n",
    "    User Query: What is the difference between ceramic\n",
    "    and porcelain? Please keep your response brief.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s2ICo_ehthYI",
   "metadata": {
    "id": "s2ICo_ehthYI"
   },
   "source": [
    "# Add contextual information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cda2bbb",
   "metadata": {},
   "source": [
    "Imagine you work for a grocery store chain and want to provide users a way of finding items in your store easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "k5s-N0X0tjEO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1890,
     "status": "ok",
     "timestamp": 1734339874017,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -420
    },
    "id": "k5s-N0X0tjEO",
    "outputId": "0a08817d-8fd4-48ac-877c-d8fc6b871ebc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Grocery Store Aisle Numbers:\n",
      "\n",
      "Here's where you can find the items you listed in a typical grocery store:\n",
      "\n",
      "* **Paper plates:** Aisle 7 (Paper goods and disposable tableware)\n",
      "* **Mustard:** Aisle 5 (Condiments and sauces)\n",
      "* **Potatoes:** Aisle 9 (Produce section) \n",
      "\n",
      "**Please note:** This is just a general guide, and the actual aisle numbers may vary depending on the specific store layout. It's always a good idea to check the store directory or ask a staff member for assistance if you can't find what you're looking for. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    On what aisle numbers can I find the following items?\n",
    "    - paper plates\n",
    "    - mustard\n",
    "    - potatoes\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "SXfQYlR3trv_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1170,
     "status": "ok",
     "timestamp": 1734339907359,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -420
    },
    "id": "SXfQYlR3trv_",
    "outputId": "4a53bc4a-5176-48f8-a0c6-af7b6cb78b68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Aisle Numbers for Your Items:\n",
      "\n",
      "* **Paper plates:** Aisle 17 (Household & Cleaning Supplies)\n",
      "* **Mustard:** Aisle 8 (Condiments & Spices)\n",
      "* **Potatoes:** Aisle 2 (Vegetables) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\"\"\"\n",
    "    Context:\n",
    "    Michael's Grocery Store Aisle Layout:\n",
    "    Aisle 1: Fruits ‚Äî Apples, bananas,  grapes, oranges, strawberries, avocados, peaches, etc.\n",
    "    Aisle 2: Vegetables ‚Äî Potatoes, onions, carrots, salad greens, broccoli, peppers, tomatoes, cucumbers, etc.\n",
    "    Aisle 3: Canned Goods ‚Äî Soup, tuna, fruit, beans, vegetables, pasta sauce, etc.\n",
    "    Aisle 4: Dairy ‚Äî Butter, cheese, eggs, milk, yogurt, etc.\n",
    "    Aisle 5: Meat‚Äî Chicken, beef, pork, sausage, bacon etc.\n",
    "    Aisle 6: Fish & Seafood‚Äî Shrimp, crab, cod, tuna, salmon, etc.\n",
    "    Aisle 7: Deli‚Äî Cheese, salami, ham, turkey, etc.\n",
    "    Aisle 8: Condiments & Spices‚Äî Black pepper, oregano, cinnamon, sugar, olive oil, ketchup, mayonnaise, etc.\n",
    "    Aisle 9: Snacks‚Äî Chips, pretzels, popcorn, crackers, nuts, etc.\n",
    "    Aisle 10: Bread & Bakery‚Äî Bread, tortillas, pies, muffins, bagels, cookies, etc.\n",
    "    Aisle 11: Beverages‚Äî Coffee, teabags, milk, juice, soda, beer, wine, etc.\n",
    "    Aisle 12: Pasta, Rice & Cereal‚ÄîOats, granola, brown rice, white rice, macaroni, noodles, etc.\n",
    "    Aisle 13: Baking‚Äî Flour, powdered sugar, baking powder, cocoa etc.\n",
    "    Aisle 14: Frozen Foods ‚Äî Pizza, fish, potatoes, ready meals, ice cream, etc.\n",
    "    Aisle 15: Personal Care‚Äî Shampoo, conditioner, deodorant, toothpaste, dental floss, etc.\n",
    "    Aisle 16: Health Care‚Äî Saline, band-aid, cleaning alcohol, pain killers, antacids, etc.\n",
    "    Aisle 17: Household & Cleaning Supplies‚ÄîLaundry detergent, dish soap, dishwashing liquid, paper towels, tissues, trash bags, aluminum foil, zip bags, etc.\n",
    "    Aisle 18: Baby Items‚Äî Baby food, diapers, wet wipes, lotion, etc.\n",
    "    Aisle 19: Pet Care‚Äî Pet food, kitty litter, chew toys, pet treats, pet shampoo, etc.\n",
    "\n",
    "    Query:\n",
    "    On what aisle numbers can I find the following items?\n",
    "    - paper plates\n",
    "    - mustard\n",
    "    - potatoes\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l8lO3TH3tt5i",
   "metadata": {
    "id": "l8lO3TH3tt5i"
   },
   "source": [
    "# Structure prompts with prefixes or tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4585c437",
   "metadata": {},
   "source": [
    "Review the following prompt for a hypothetical text-based dating application. It contains several prompt components, including:\n",
    "- defining a persona\n",
    "- specifying instructions\n",
    "- providing multiple pieces of contextual information for the main user and potential matches.\n",
    "Notice how the XML-style tags (like <OBJECTIVE_AND_PERSONA>) divide up sections of the prompt and other prefixes like Name: identify other key pieces of information. This allows for complex structure within a prompt while keeping each section clearly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "NF5DChAotu1A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2286,
     "status": "ok",
     "timestamp": 1734339952949,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -420
    },
    "id": "NF5DChAotu1A",
    "outputId": "43ad8759-892d-467f-9375-55ef88079060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## You should totally check out Felix! \n",
      "\n",
      "He's also really into classical music, especially Beethoven, and even makes a mean German pasta dish called spaetzle. Plus, he used to play water polo and still loves spending time by the beach, just like you love to swim. You two could spend hours discussing your favorite composers, enjoying delicious Italian and German food, and having fun by the water. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "  <OBJECTIVE_AND_PERSONA>\n",
    "  You are a dating matchmaker.\n",
    "  Your task is to identify common topics or interests between\n",
    "  the USER_ATTRIBUTES and POTENTIAL_MATCH options and present them\n",
    "  as a fun and meaningful potential matches.\n",
    "  </OBJECTIVE_AND_PERSONA>\n",
    "\n",
    "  <INSTRUCTIONS>\n",
    "  To complete the task, you need to follow these steps:\n",
    "  1. Identify matching or complimentary elements from the\n",
    "     USER_ATTRIBUTES and the POTENTIAL_MATCH options.\n",
    "  2. Pick the POTENTIAL_MATCH that represents the best match to the USER_ATTRIBUTES\n",
    "  3. Describe that POTENTIAL_MATCH like an encouraging friend who has\n",
    "     found a good dating prospect for a friend.\n",
    "  4. Don't insult the user or potential matches.\n",
    "  5. Only mention the best match. Don't mention the other potential matches.\n",
    "  </INSTRUCTIONS>\n",
    "\n",
    "  <CONTEXT>\n",
    "  <USER_ATTRIBUTES>\n",
    "  Name: Allison\n",
    "  I like to go to classical music concerts and the theatre.\n",
    "  I like to swim.\n",
    "  I don't like sports.\n",
    "  My favorite cuisines are Italian and ramen. Anything with noodles!\n",
    "  </USER_ATTRIBUTES>\n",
    "\n",
    "  <POTENTIAL_MATCH 1>\n",
    "  Name: Jason\n",
    "  I'm very into sports.\n",
    "  My favorite team is the Detroit Lions.\n",
    "  I like baked potatoes.\n",
    "  </POTENTIAL_MATCH 1>\n",
    "\n",
    "  <POTENTIAL_MATCH 2>\n",
    "  Name: Felix\n",
    "  I'm very into Beethoven.\n",
    "  I like German food. I make a good spaetzle, which is like a German pasta.\n",
    "  I used to play water polo and still love going to the beach.\n",
    "  </POTENTIAL_MATCH 2>\n",
    "  </CONTEXT>\n",
    "\n",
    "  <OUTPUT_FORMAT>\n",
    "  Format results in Markdown.\n",
    "  </OUTPUT_FORMAT>\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SPpY95Vjt8yS",
   "metadata": {
    "id": "SPpY95Vjt8yS"
   },
   "source": [
    "# Use system instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "jiZzuGgDt9yW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 117409,
     "status": "ok",
     "timestamp": 1734340107080,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -420
    },
    "id": "jiZzuGgDt9yW",
    "outputId": "657ca26a-0c82-4e0c-cfc9-8a6db1defeec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh my, are you asking about musicians worth studying? Because in the vast and vibrant world of music, there are so many incredible artists who offer unique insights, innovations, and inspirations!  \n",
      "\n",
      "Do you have a particular genre in mind? For example, studying the Baroque giant **Johann Sebastian Bach** will deepen your understanding of counterpoint and fugue.  Or perhaps you're curious about the development of jazz?  In that case, exploring the groundbreaking work of **Louis Armstrong** or **Billie Holiday** would be fascinating.  \n",
      "\n",
      "Tell me, what kind of music makes your heart sing?  From there, we can delve into the artists who shaped that sound! üòä üé∂ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_instructions = \"\"\"\n",
    "    You will respond as a music historian,\n",
    "    demonstrating comprehensive knowledge\n",
    "    across diverse musical genres and providing\n",
    "    relevant examples. Your tone will be upbeat\n",
    "    and enthusiastic, spreading the joy of music.\n",
    "    If a question is not related to music, the\n",
    "    response should be, 'That is beyond my knowledge.'\n",
    "\"\"\"\n",
    "\n",
    "music_model = GenerativeModel(\"gemini-1.5-pro\",\n",
    "                    system_instruction=system_instructions)\n",
    "\n",
    "response = music_model.generate_content(\n",
    "    \"\"\"\n",
    "    Who is worth studying?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R9QEvNR_uJFg",
   "metadata": {
    "id": "R9QEvNR_uJFg"
   },
   "source": [
    "# Demonstrate Chain-of-Thought"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51251f06",
   "metadata": {},
   "source": [
    "Large language models predict what language should follow other language, but they cannot think through cause and effect in the world outside of language. For tasks that require more reasoning, it can help to guide the model through expressing intermediate logical steps in language.\n",
    "\n",
    "Large Language Models, especially Gemini, have gotten much better at reasoning on their own. But they can sometimes still use guidance to assist in laying out one logical step at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "kuuMNGMuuLor",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2746,
     "status": "ok",
     "timestamp": 1734340143829,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -420
    },
    "id": "kuuMNGMuuLor",
    "outputId": "5748576e-936c-4f9f-e4e3-072c9041ed64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Today's Production:\n",
      "\n",
      "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
      "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
      "* **Total production today: 300 units/day + 60 units/day = 360 units/day**\n",
      "\n",
      "## Tomorrow's Production:\n",
      "\n",
      "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
      "* Medium efficiency factories: 1 factory * 60 units/day/factory (reconfigured) + 1 factory * 30 units/day/factory (half output) = 90 units/day\n",
      "* Low efficiency factories: 1 factory * 15 units/day/factory (half output) = 15 units/day\n",
      "* **Total production tomorrow: 300 units/day + 90 units/day + 15 units/day = 405 units/day** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "Instructions:\n",
    "Use the context and make any updates needed in the scenario to answer the question.\n",
    "\n",
    "Context:\n",
    "A high efficiency factory produces 100 units per day.\n",
    "A medium efficiency factory produces 60 units per day.\n",
    "A low efficiency factory produces 30 units per day.\n",
    "\n",
    "Megacorp owns 5 factories. 3 are high efficiency, 2 are low efficiency.\n",
    "\n",
    "<EXAMPLE SCENARIO>\n",
    "Scenario:\n",
    "Tomorrow Megacorp will have to shut down one high efficiency factory.\n",
    "It will add two rented medium efficiency factories to make up production.\n",
    "\n",
    "Question:\n",
    "How many units can they produce today? How many tomorrow?\n",
    "\n",
    "Answer:\n",
    "\n",
    "Today's Production:\n",
    "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
    "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
    "* **Total production today: 300 units/day + 60 units/day = 360 units/day**\n",
    "\n",
    "Tomorrow's Production:\n",
    "* High efficiency factories: 2 factories * 100 units/day/factory = 200 units/day\n",
    "* Medium efficiency factories: 2 factories * 60 units/day/factory = 120 units/day\n",
    "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
    "* **Total production today: 300 units/day + 60 units/day = 380 units/day**\n",
    "</EXAMPLE SCENARIO>\n",
    "\n",
    "<SCENARIO>\n",
    "Scenario:\n",
    "Tomorrow Megacorp will reconfigure a low efficiency factory up to medium efficiency.\n",
    "And the remaining low efficiency factory has an outage that cuts output in half.\n",
    "\n",
    "Question:\n",
    "How many units can they produce today? How many tomorrow?\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "response = model.generate_content(question,\n",
    "                                  generation_config={\"temperature\": 0})\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Iw3RtlXDumEX",
   "metadata": {
    "id": "Iw3RtlXDumEX"
   },
   "source": [
    "# Break down complex tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f295cc",
   "metadata": {},
   "source": [
    "Often, complex tasks require multiple steps to work through them, even for us humans! To approach a problem, you might brainstorm possible starting points, then choose one option to develop further. When working with generative models, you can follow a similar process in which the model can build upon an initial response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "yhniVBeburIc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7100,
     "status": "ok",
     "timestamp": 1734340180883,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -420
    },
    "id": "yhniVBeburIc",
    "outputId": "011f10a1-3896-4d4d-cfa3-23676ad9d139"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 5 Metaphors Comparing TPUs and GPUs:\n",
      "\n",
      "1. **TPU as a Specialized Athlete, GPU as a Decathlete:** \n",
      "    * A **TPU** is like a specialized athlete, excelling in one specific task, like sprinting. It's highly efficient and optimized for that task, performing it with incredible speed.\n",
      "    * A **GPU** is like a decathlete, capable of handling a variety of tasks, from running to swimming to jumping. It's versatile and adaptable, but may not be as fast as a specialized athlete in any one area.\n",
      "\n",
      "2. **TPU as a Formula 1 Car, GPU as a Sports Car:**\n",
      "    * A **TPU** is like a Formula 1 car, built for raw speed and optimized for one purpose: racing. It excels in specific tasks with incredible efficiency and performance.\n",
      "    * A **GPU** is like a sports car, powerful and capable of handling various tasks, from cruising on the highway to tackling winding mountain roads. It's versatile and adaptable, but may not be as fast as a Formula 1 car in a straight race.\n",
      "\n",
      "3. **TPU as a Laser, GPU as a Swiss Army Knife:**\n",
      "    * A **TPU** is like a laser, focused and intense, delivering incredible power to a specific point. It excels in tasks requiring high computational power for specific operations.\n",
      "    * A **GPU** is like a Swiss Army Knife, versatile and equipped with various tools for different tasks. It can handle diverse workloads, from running simulations to editing videos.\n",
      "\n",
      "4. **TPU as a Scalpel, GPU as a Kitchen Knife:**\n",
      "    * A **TPU** is like a scalpel, precise and specialized, ideal for performing delicate, intricate tasks requiring high accuracy. It excels in tasks like machine learning and neural network training.\n",
      "    * A **GPU** is like a kitchen knife, multipurpose and capable of handling various tasks, from chopping vegetables to slicing meat. It can handle diverse workloads, from gaming to scientific simulations.\n",
      "\n",
      "5. **TPU as a Factory Assembly Line, GPU as a Workshop:**\n",
      "    * A **TPU** is like a factory assembly line, highly efficient and optimized for mass production of a specific product. It excels in tasks requiring high throughput and repetitive operations.\n",
      "    * A **GPU** is like a workshop, equipped with various tools and capable of handling diverse tasks, from building furniture to fixing electronics. It can handle various workloads, from creative tasks to scientific calculations.\n",
      "\n",
      "These metaphors highlight the key differences between TPUs and GPUs, emphasizing their strengths and limitations. Choosing the right tool for the job depends on the specific requirements and desired outcome.\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    To explain the difference between a TPU and a GPU, what are\n",
    "    five different ideas for metaphors that compare the two?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "brainstorm_response = response.text\n",
    "print(brainstorm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "BphyycHkuyY6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4882,
     "status": "ok",
     "timestamp": 1734340198868,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -420
    },
    "id": "BphyycHkuyY6",
    "outputId": "57e71324-219d-44cc-89d5-fd2634434690"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Formula 1 Car vs. Sports Car Analogy: Capturing the Essence of TPUs and GPUs\n",
      "\n",
      "The analogy comparing a **TPU** to a **Formula 1 car** and a **GPU** to a **sports car** resonates deeply with my understanding of these technologies. It effectively conveys their strengths and limitations in a way that is both visually engaging and conceptually clear.\n",
      "\n",
      "This analogy highlights the **specialized nature of TPUs**, much like a Formula 1 car meticulously engineered for raw speed on the track. TPUs excel in specific tasks, particularly those requiring **high computational power** for **repetitive operations**. Just like a Formula 1 car wouldn't be ideal for navigating city streets, TPUs are not as well-suited for handling diverse workloads.\n",
      "\n",
      "On the other hand, the **GPU is portrayed as a versatile sports car**, capable of handling a variety of tasks with relative ease. Just like a sports car can navigate both city streets and open highways, GPUs can handle diverse workloads, from **scientific simulations** to **gaming** to **video editing**. While not as specialized as a TPU, the GPU's adaptability makes it a valuable tool for a broader range of applications.\n",
      "\n",
      "Moreover, the analogy underscores the **trade-off between specialization and versatility**. Choosing between a TPU and GPU hinges on the specific task at hand. If the task demands **maximum performance** for a particular operation, a TPU is the clear winner. However, if the task requires **adaptability and handling diverse workloads**, a GPU offers greater flexibility.\n",
      "\n",
      "Ultimately, this analogy helps me visualize the fundamental differences between TPUs and GPUs, aiding my understanding of their capabilities and limitations. It reminds me that the **best choice** depends on the **specific task** and the **desired outcome**. The Formula 1 car and sports car analogy effectively encapsulates this critical insight.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    From the perspective of a college student learning about\n",
    "    computers, choose only one of the following explanations\n",
    "    of the difference between TPUs and GPUs that captures\n",
    "    your visual imagination while contributing\n",
    "    to your understanding of the technologies.\n",
    "\n",
    "    {brainstorm_response}\n",
    "    \"\"\".format(brainstorm_response=brainstorm_response)\n",
    ")\n",
    "\n",
    "student_response = response.text\n",
    "\n",
    "print(student_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ko0C0LeCu1NK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4689,
     "status": "ok",
     "timestamp": 1734340212102,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -420
    },
    "id": "ko0C0LeCu1NK",
    "outputId": "11e5e387-92ab-4331-deee-f5875ef1c241"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Formula 1 Car vs. Sports Car Analogy: Understanding TPUs and GPUs\n",
      "\n",
      "Ever wondered what the difference is between a TPU and a GPU? The answer can be as simple as picturing the difference between a Formula 1 car and a sports car. Both are powerful machines, designed for speed and performance, but they each serve distinct purposes.\n",
      "\n",
      "TPUs, like the meticulously engineered Formula 1 car, are built for **pure speed on a specific track**. They excel in **repetitive, computationally intensive tasks**, delivering unmatched performance when it comes to crunching massive amounts of data. However, just like a Formula 1 car wouldn't fare well on a bumpy city street, TPUs struggle with diverse workloads that require flexibility.\n",
      "\n",
      "GPUs, on the other hand, are the versatile sports cars of the computational world. They can handle **a variety of tasks** with relative ease, from **scientific simulations** to **gaming** to **video editing**. While they may not reach the raw speed of a TPU on a specific track, their adaptability makes them a valuable tool for a broader range of applications.\n",
      "\n",
      "This analogy highlights the crucial **trade-off between specialization and versatility**. Choosing between a TPU and a GPU depends entirely on the task at hand. If you need the **absolute top speed** for a specific operation, the TPU is your champion. But if your workload demands **handling diverse tasks** with agility, the GPU emerges as the more flexible choice.\n",
      "\n",
      "Ultimately, the Formula 1 car and sports car analogy provides a clear and engaging way to understand the fundamental differences between TPUs and GPUs. It emphasizes that the **best choice** hinges on the **specific requirements of your task**, reminding us to select the tool that will best help us reach the finish line. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    Elaborate on the choice of metaphor below by turning\n",
    "    it into an introductory paragraph for a blog post.\n",
    "\n",
    "    {student_response}\n",
    "    \"\"\".format(student_response=student_response)\n",
    ")\n",
    "\n",
    "blog_post = response.text\n",
    "\n",
    "print(blog_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WusPH3pmu465",
   "metadata": {
    "id": "WusPH3pmu465"
   },
   "source": [
    "# Implement prompt iteration strategies to improve your prompts version by version\n",
    "\n",
    "Your prompts may not always generate the results you have imagined on your first attempt.\n",
    "\n",
    "A few steps you can take to iterate on your prompts include:\n",
    "\n",
    "Rephrasing the descriptions of your task, instructions, persona, or other prompt components.\n",
    "Re-ordering the various components of the prompt to give the model a clue as early as possible as to what parts of the text you have provided are most relevant.\n",
    "Breaking your task up into multiple, smaller tasks."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "student-00-79ceaeefde38 (Dec 16, 2024, 3:41:52‚ÄØPM)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
